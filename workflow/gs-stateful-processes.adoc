= Getting started with process automation
include::../partials/attributes.adoc[]

This section describes how to configure a stateful process project in {PRODUCT_SHORT}. It refers to the sample project demonstrating a stateful hiring process service: xref:../getting-started/examples.html[jbpm-compact-architecture-example], which drives a candidate through different interviews to the successful hiring decision. The example includes the use of DMN decisions to generate the candidate offer and timers to skip user tasks. 

== Creating and configuring a stateful process

In this section we will focus on how to build and configure the Hiring process in Canvas.

For information on setting up Canvas see xref:../installation/canvas.adoc[Canvas installation]

=== Create Process Variables

. Open Canvas with workflow elements.

. Click anywhere in the canvas and then click on the properties tab to access the properties panel.
+
.Canvas for workflow showing properties
image::wf-tutorial/canvas-wf-1.png[Canvas for workflow showing properties]

. Set the following properties:
 * *Name*: Provide a label for the process.
 * *ID*: Specify an identifier that will be used for endpoints.
 * *Description*: Write a detailed description of the process.
. To add input variables:
 * Click the plus button to add a new variable.
 * Set the variable name and data type. 
 ** _Name_ is the ID and name of the variable which can be referenced via scripts or the input/output of activities. 
 ** Data type refers to a Java type. If you click Select Custom, the field turns into the fully qualified name of the class. 

.Process properties
image::wf-tutorial/input-data-2.png[Setting process properties]

=== Define Workflow Elements

Use the following elements to define your workflow:

* *Start Event*: Green circle indicating the starting point of the workflow.
* *Intermediate Events*: Yellow circle for events that occur between the start and end points.
* *End Events*: Red circle marking the end of the workflow.
* *Activities*: Empty rectangle representing tasks or activities in the workflow.
* *Subprocess*: Rectangle with a square for subprocesses.
* *Gateways*: Diamond shape for decision points.
* *Lanes*: Checklist icon to organize tasks by roles or departments.
* *Non-executable Items*: 3D cube for items not executed within the workflow.
* *Miscellaneous*: Engine icon for other workflow components.

=== Adjust Properties and Add Process Variables

. Create a start node with a script to initialize variables:
// where does the script need to go - in UI or javascript called via UI?
+
[source,java]
----
System.out.println("New Hiring has been created for candidate: " + candidateData.getFullName());
kcontext.setVariable("hr_approval", false);
kcontext.setVariable("it_approval", false);
----
. Add an exclusive gateway for decision-making.
. Create a business rule task with specific properties and data assignments:
+
[source,java]
----
System.out.println("###################################");
System.out.println("Generated offer for candidate: " + candidateData.getFullName());
System.out.println("Job Category: " + offer.getCategory());
System.out.println("Base salary: " + offer.getSalary());
System.out.println("###################################");
----
. Create and configure human tasks and boundary events:
+
[source,java]
----
System.out.println("###################################");
System.out.println("HR Interview have been avoided after reasonable time");
System.out.println("###################################");
----
. Add scripts for various tasks and events to log important details and actions:
+
[source,java]
----
System.out.println("###################################");
System.out.println("To: " + candidateData.getEmail());
System.out.println("Subject: Congratulations you made it!");
System.out.println("Dear " + candidateData.getFullName() + ", we are happy to tell you that you've successfully went through the hiring process. You'll find the final Offer details in attached.");
System.out.println("Job Category: " + offer.getCategory());
System.out.println("Base salary: " + offer.getSalary());
System.out.println("###################################");

System.out.println("###################################");
System.out.println("Candidate " + candidateData.getFullName() + " doesn't meet the requirements for the position but we'll keep it on records for the future!");
System.out.println("###################################");
----

== Creating a new project using accelerators in Canvas
The accelerator functionality available in Canvas can be used to quickly create the required artifacts for a stateful process service. See xref:../installation/canvas.adoc[installing Canvas] for more information on setting up Canvas. 

. Open Canvas.
. Import `hiring.bpmn` (xref:hiring.bpmn.zip[sample hiring BPMN file]).
. Click *Apply accelerator* -> *Quarkus (Full)*, and on the pop-up click *Apply*.
+
.New process project with accelerator
image::wf-tutorial/1-new-process-project-with-accelerator.png[Process project with accelerator]
+
. Click "Untitled Project" and rename the project. In this example, the project 1. is named `process-service-sample`
+
.Renaming project
image::wf-tutorial/2-renaming-project.png[Renaming project]
. Download the project to your local development environment by clicking on *Share* -> *All files*. This enables you to run the project locally with BAMOE Developer tools for VS Code. 

NOTE: Users with pre-configured git authentication can synchronize the new project on git and push it to a new repository.



== Running the project locally with VSCode

Following the download of your process service to your local development environment, you can unzip the project into a new folder using the following command:

`$ unzip process-service-sample.zip -d process-service-sample`

Open the project `process-service-sample` using VSCode.

When using IBM BAMOE Developer Tools extension, developers can seamlesly interact with business assets directly from the developer IDE.

.Project in VS Code
image::wf-tutorial/3-project-in-vscode.png[Project in VS Code]

Note that the project includes the process definition and the `application.properties` file with generic sample configurations:

[source,properties]
----
# Set up for the swagger-ui (Process-defintions)
quarkus.http.cors=true
quarkus.smallrye-openapi.path=/docs/openapi.json
quarkus.swagger-ui.always-include=true
kogito.generate.rest.decisions=true
kogito.generate.rest.processes=true

# list of users for task console
quarkus.kogito-runtime-tools.users=jdoe,admin,user
quarkus.kogito-runtime-tools.users.admin.groups=admin
quarkus.kogito-runtime-tools.users.user.groups=user
quarkus.kogito-runtime-tools.users.jdoe.groups=user,admin,IT,HR
kogito.service.url=http://localhost:8080
kogito.dataindex.http.url=http://localhost:8180
kogito.dataindex.ws.url=ws://localhost:8180

# Maximum Java heap to be used during the native image generation
quarkus.native.native-image-xmx=6g
kafka.bootstrap.servers=localhost:9092

## main transport

## metadata

#mp.messaging.outgoing.kogito-processinstances-events.bootstrap.servers=localhost:9092
mp.messaging.outgoing.kogito-processinstances-events.connector=smallrye-kafka
mp.messaging.outgoing.kogito-processinstances-events.topic=kogito-processinstances-events
mp.messaging.outgoing.kogito-processinstances-events.value.serializer=org.apache.kafka.common.serialization.StringSerializer
#mp.messaging.outgoing.kogito-usertaskinstances-events.bootstrap.servers=localhost:9092
mp.messaging.outgoing.kogito-usertaskinstances-events.connector=smallrye-kafka
mp.messaging.outgoing.kogito-usertaskinstances-events.topic=kogito-usertaskinstances-events
mp.messaging.outgoing.kogito-usertaskinstances-events.value.serializer=org.apache.kafka.common.serialization.StringSerializer
#mp.messaging.outgoing.kogito-variables-events.bootstrap.servers=localhost:9092
mp.messaging.outgoing.kogito-variables-events.connector=smallrye-kafka
mp.messaging.outgoing.kogito-variables-events.topic=kogito-variables-events
mp.messaging.outgoing.kogito-variables-events.value.serializer=org.apache.kafka.common.serialization.StringSerializer
----

To execute the project in the local development environment, open a new terminal in VScode and execute:

[source,bash]
----
mvn clean package
----

.Opening a terminal in VS Code
image::wf-tutorial/4-new-terminal-vscode.png[Opening a terminal in VS Code]

This command should compile and package the process service into a Quarkus executable Jar file. The resulting package is the service that can later be containerized and executed on cloud platforms. The process service generated using the accelerator brings a pre-configured set of add-ons via the `pom.xml` file. You can adapt this file to meet the requirements of your service.

The first execution downloads any artifacts not yet available on the development environment

== Enabling JDBC Persistence for {BAMOE} stateful processes

{BAMOE} process service and all the {BAMOE} subsystems (Data Index, Data Audit, Job Service and User Tasks) use JDBC to persist
 in relational databases. The supported databases are PostgreSQL and Microsoft SQL Server for production environments
and H2 for development.

All {BAMOE} components will rely on the default DataSource to establish the connection with the connection with
the Data Base, so it is mandatory that the applications has a DataSource correctly configured following the platform (Quarkus)
guidelines.

Configuring persistence for {BAMOE} requires a minimal  set of dependencies and configurations in your application. The
minimal set of dependencies are:

* Quarkus Agroal (_io.quarkus:quarkus-agroal_): Quarkus DataSource Connection Pool with Security and Transaction support
* Database Specific Quarkus JDBC Driver (_io.quarkus:quarkus-jdbc-<db-type>_)
* KIE Add-On Persistence JDBC (_org.kie:kie-addons-quarkus-persistence-jdbc_): Enables JDBC persistence for the process engine.

NOTE: For full information about how to configure DataSources in Quarkus, please check the
https://quarkus.io/version/3.15/guides/datasource#jdbc-configuration[Configure data sources in Quarkus] guide.

=== Configuring JDBC Persistence for PostgreSQL

To configure PostgreSQL persistence for your {BAMOE} application, in your `pom.xml` add the following dependencies:
[source,xml]
----
<dependencies>
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-agroal</artifactId>
    </dependency>
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-jdbc-postgresql</artifactId>
    </dependency>
    <dependency>
        <groupId>org.kie</groupId>
        <artifactId>kie-addons-quarkus-persistence-jdbc</artifactId>
    </dependency>
</dependencies>
----

Configuration required in your `application.properties`:
[source,properties]
----
# Application default Datasource - PostgreSQL
quarkus.datasource.db-kind=postgresql
quarkus.datasource.username=bamoe-user
quarkus.datasource.password=bamoe-pass
quarkus.datasource.jdbc.url=${QUARKUS_DATASOURCE_JDBC_URL:jdbc:postgresql://0.0.0.0:5432/bamoe-db}

# Enabling jdbc persistence for kogito processes
kogito.persistence.type=jdbc
----

=== Configuring JDBC Persistence for Microsoft SQL Server

To configure Microsoft SQL Server persistence for your {BAMOE} application, in your `pom.xml` add the following dependencies:
[source,xml]
----
<dependencies>
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-agroal</artifactId>
    </dependency>
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-jdbc-mssql</artifactId>
    </dependency>
    <dependency>
        <groupId>org.kie</groupId>
        <artifactId>kie-addons-quarkus-persistence-jdbc</artifactId>
    </dependency>
</dependencies>
----

Configuration required in your `application.properties`:
[source,properties]
----
# Application default Datasource - Microsoft SQL Server
quarkus.datasource.db-kind=mssql
quarkus.datasource.username=bamoe-user
quarkus.datasource.password=bamoe-pass
quarkus.datasource.jdbc.url=${QUARKUS_DATASOURCE_JDBC_URL:jdbc:sqlserver://0.0.0.0:1433;DatabaseName=bamoe-db;encrypt=true;trustServerCertificate=true;}

# Enabling jdbc persistence for kogito processes
kogito.persistence.type=jdbc
----

==== {BAMOE} Microsoft SQL Server Mappings

Additionally, if the {BAMOE} application includes the Data Index or Job Service, it's also required to add the `bamoe-mssql-mappings` dependency
to ensure the correct behaviour of those components. This module includes the necessary Hibernate ORM mappings to help remapping
some of the JPA entities used in both components. The mappings contained are:

* META-INF/bamoe-data-index-orm.xml: This file remaps some entities from the data-index component.
* META-INF/bamoe-job-service-orm.xml: This file remaps some entities from the job-service component.

Those mappings can be configured by using the `quarkus.hibernate-orm.mapping-files` configuration, and each one should be
added only if the associated component is present in the application dependencies.

To correctly configure the Hibernate mappings, in your application `pom.xml` add:
[source,xml]
----
<dependency>
    <groupId>com.ibm.bamoe</groupId>
    <artifactId>bamoe-mssql-mappings</artifactId>
    <version>${version.com.ibm.bamoe}</version>
</dependency>
----

And in the `application.properties`:
[source,properties]
----
quarkus.hibernate-orm.mapping-files=META-INF/bamoe-data-index-orm.xml,META-INF/bamoe-job-service-orm.xml
----

=== Configuring JDBC Persistence for H2 (Only for development)

In Development environments, you can use H2 persistence for your {BAMOE} application, in your `pom.xml` add the following dependencies:
[source,xml]
----
<dependencies>
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-agroal</artifactId>
    </dependency>
    <dependency>
        <groupId>io.quarkus</groupId>
        <artifactId>quarkus-jdbc-h2</artifactId>
    </dependency>
    <dependency>
        <groupId>org.kie</groupId>
        <artifactId>kie-addons-quarkus-persistence-jdbc</artifactId>
    </dependency>
</dependencies>
----

Configuration required in your `application.properties`:
[source,properties]
----
# Application default Datasource - H2
quarkus.datasource.db-kind=h2
datasource.devservices.properties.NON_KEYWORDS=VALUE,KEY

# Enabling jdbc persistence for kogito processes
kogito.persistence.type=jdbc
----

=== Configuring the process runtime database

When deploying process service, you will need to configure the database so that the process runtime can persist data such as User Tasks, Audit logs, etc. To do that, {PRODUCT_SHORT} comes with a Flyway-based mechanism that automatically creates the database schema during startup. This is convenient in a development environment and is not adapted to production database instances.

A set of DDL (Data Definition Language) scripts for both supported databases (PostgreSQL and MS SQL Server) is included in the delivery (`bamoe-9.2.0-db-utils.zip`) to enable a Database administrator to manually execute them as required on a production database, in order to create the database to be used by the persistence layer of the following subsystems (see xref:deploying-process-services.adoc[The components of a process service]):

* Data Index
* Data Audit
* Job Services
* Runtime Engine
* User Tasks

The script creates all the tables required by the features listed above to work.

The scripts can be executed in any order, and the required tables will be created in the database that will be used by the {BAMOE} process service. 

In Development environments, you can use Kie Flyway to automate the process of creating and maintaining database tables across patch releases.
To avoid manual management of the above scripts and their future update. To enable this feature, the following property
should be added in your project `application.property` file, which is found in `application.properties`:

* kie.flyway.enabled=true 

The default is false.

If everything is correctly set, at the first project execution all the tables will be automatically added.

The following sections contain a description of all tables that are created by the DDL script. 

=== Process Persistence tables

[%autowidth]
|===
|Table |Description

|business_key_mapping 
| 

|correlation_instances 
|composite business keys to locate a process without the process instance id

|process_instances
|process state from alive processes
|===

=== Data-index tables

[%autowidth]
|===
|Table |Description

|attachments  
|user task instance attachments belongs

|comments  
|user task instance comments belongs

|definitions 
|process definitions that have been deployed

|definitions_addons  
|addons the process definitions were deployed with

|definitions_annotations 
|annotations the process definitions were deployed with

|definitions_metadata 
|metadata the process definitions were deployed with

|definitions_nodes
|last definitions of node executed by a process instance 

|definitions_nodes_metadata
|metadata the last definitions of node executed by a process instance was deployed with

|definitions_roles
|roles the process definitions were deployed with

|jobs
|timers created by runtime

|kogito_data_cache
|The data cache used by Kogito

|milestones
|special type of node that is completed through a condition (comes from cmmn)

|nodes 
|nodes executed by the process instance

|processes 
|last state of the process instance

|processes_addons 
|addons this process instance is being executed with

|processes_roles 
|roles this process instance is required

|tasks 
|user task instance last state

|tasks_admin_groups 
|user task instance admin groups assinged

|tasks_admin_users 
|user task instance admin user assingned

|tasks_excluded_users
|user task instance excluded users

|tasks_potential_groups
|user task instance potential groups

|tasks_potential_users
|user task instance potential users
|===

=== Job Service tables

[%autowidth]
|===
|Table |Description

|job_details  
| job instance being created

|job_service_management 
|use for clustering and to check which instance is the lead
|===

=== Data Audit tables

[%autowidth]
|===
|Table |Description

|audit_query 
|store custom queries against data audit tables

|job_execution_log 
|historical records of events of job execution

|process_instance_error_log 
|historical record of process instance errors

|process_instance_node_log 
|historical record of node instance executions

|process_instance_state_log 
|historical record of node state changed during executions

|process_instance_state_roles_log 
|historical record of process instance state changed during execution

|process_instance_variable_log 
|historical record of varaible changes during process instance execution

|task_instance_assignment_log 
|historical record of assignments in user task instance

|task_instance_assignment_users_log 
|historical record of assignments in user task instance

|task_instance_attachment_log 
|historical record of user task instance attachments

|task_instance_comment_log 
|historical record of user task instance comments

|task_instance_deadline_log 
|historical record of user task instance deadlines change

|task_instance_deadline_notification_log 
|historical record of use task instance deadlines notifications

|task_instance_state_log 
|historical record of user task instance state change

|task_instance_variable_log
|historical record of user task instance input/output variables change
|===

=== User tasks tables
[%autowidth]
|===
|Table |Description

|jbpm_user_tasks  
|The entity that represents a User task

|jbpm_user_tasks_potential_users 
|The potential users assignable to a given User task

|jbpm_user_tasks_potential_groups
|The potential groups assignable to a given User Task

|jbpm_user_tasks_admin_users
|User Admins assigned to a given User Task

|jbpm_user_tasks_admin_group
|Group Admins assigned to a given User Task

|jbpm_user_tasks_excluded_users
|Users which canâ€™t be assigned to a given User Task

|jbpm_user_tasks_attachments
|An attachment is a reference to an external URI containing information relevant to a related task

|jbpm_user_tasks_comments
|A comment consists of a human readable text that will help to achieve a successful resolution of a User Task

|jbpm_user_tasks_inputs
|Input parameters of a User Task which are passed as a pair (name, value) format, to be consumed by a Human

|jbpm_user_tasks_ouputs
|Output parameters of a User Task which results in a set of properties in a pair (name, value) format

|jbpm_user_tasks_metadata
|Global properties related to the User Task
|===